{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "Author: Oliver Giesecke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import os, shutil\n",
    "import re\n",
    "import csv\n",
    "from utils import bigrams, trigram, replace_collocation\n",
    "from tika import parser\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import Word2Vec\n",
    "from data_concatenate import *\n",
    "import gensim.downloader\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "DATAPATH = os.path.expanduser(\"~/Dropbox/MPCounterfactual/src/etm/\")        \n",
    "OVERLEAF = os.path.expanduser(\"~/Dropbox/Apps/Overleaf/FOMC_Summer2019/files\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 Data Preparation\n",
    "\n",
    "The training of the model requires potentially two datasets:\n",
    "1. corpus of documents for the training of word embeddings,\n",
    "2. pre-processed speaker data from the transcripts (Greenspan era).\n",
    "\n",
    "The former is used to train word embedding in isolation on a larger corpus than the what is used for the training of topics. Thus far, the datset consists of transcripts (for the entire period from 1976-2013) + all bluebooks + statements, shortly denoted by ```BBTSST```. Latter is used for the training of topics.\n",
    "\n",
    "Apart from the exact scope of the corpus, the data pre-processing requires to make a few important choices. In particular, it requires to specify:\n",
    "- treatment of stop words,\n",
    "- whether collocations are being formed (parameter: ```threshold```),\n",
    "- number of tokens that form collocation (parameter: ```phrase_itera```),\n",
    "- treatment very frequent and infrequent words (parameter: ```min_df``` and ```max_df```)\n",
    "\n",
    "The data pre-processing is implemented in the module ```data_concatenate``` and can either be called or executed independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.7 # in a maximum of # % of documents if # is float.\n",
    "min_df = 10  # choose desired value for min_df // in a minimum of # documents\n",
    "phrase_itera = 2 # Number o fphrase iterations\n",
    "threshold = \"inf\" # Threshold value for collocations. If \"inf\": no collocations\n",
    "\n",
    "print(\"Build datasets\")\n",
    "build_embdata(max_df,min_df,phrase_itera,threshold)\n",
    "build_speakerdata(max_df,min_df,phrase_itera,threshold)\n",
    "print(\"Datasets complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Train Word Embeddings\n",
    "\n",
    "We follow Mikolov et. al (2013) to train word embeddings.   \n",
    "\n",
    "Subsequently we save the model and word vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPEAKERS_10_iter2_thinf',\n",
      " 'BBTSST_10_iter2_th80',\n",
      " 'BBTSST_10_iter2_thinf',\n",
      " 'SPEAKERS_10_iter2_th80']\n"
     ]
    }
   ],
   "source": [
    "# Datasets available:\n",
    "pp.pprint([file for file in os.listdir(f\"{DATAPATH}/data/\") if file!=\".DS_Store\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run model: BBTSST_10_iter2_thinf\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "# Select corpus\n",
    "corpus = 'BBTSST_10_iter2_thinf'\n",
    "\n",
    "# Run Skipgram\n",
    "print(f\"Run model: {corpus}\")\n",
    "os.system(f\"python skipgram_man.py --data_file {DATAPATH}/data/{corpus}/corpus.pkl --modelfile {DATAPATH}/word2vecmodels/{corpus} --emb_file {DATAPATH}/embeddings/{corpus}_emb --dim_rho 300 --iters 50 --window_size 4\")\n",
    "print(f\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Evaluate Word Embeddings \n",
    "This is a visual inspection of the word vectors for different models for common domain specific terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBTSST_10_iter2_th10.model', 'BBTSST_10_iter2_thinf.model',\n",
      " 'BBTSST_10_iter2_th50.model', 'TS_10_iter2_th2.model',\n",
      " 'BBTSST_10_iter2_th2.model', 'BBTS_10_iter2_th2.model',\n",
      " 'BBTSST_10_iter2_th80.model']\n"
     ]
    }
   ],
   "source": [
    "# Available models\n",
    "pp.pprint([file for file in os.listdir(f\"{DATAPATH}/word2vecmodels/\") if file!=\".DS_Store\" and re.search(\"model$\",file)])\n",
    "# Select models\n",
    "man_models = ['BBTSST_10_iter2_thinf',\"BBTSST_10_iter2_th80\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model\n",
    "#pp.pprint(list(gensim.downloader.info()['models'].keys()))\n",
    "sel_mod = \"glove-wiki-gigaword-300\"\n",
    "glove_vectors = gensim.downloader.load(sel_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use following models:\n",
      "['BBTSST_10_iter2_thinf', 'BBTSST_10_iter2_th80', 'glove-wiki-gigaword-300']\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "models = []\n",
    "for mod in man_models:\n",
    "    models.append(gensim.models.Word2Vec.load(f\"{DATAPATH}/word2vecmodels/{mod}.model\").wv)\n",
    "\n",
    "# All models\n",
    "model_title = man_models + [sel_mod]\n",
    "models = models + [glove_vectors]\n",
    "print(\"Use following models:\")\n",
    "pp.pprint(model_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "BBTSST_10_iter2_thinf Word Vectors\n",
      "********************************************************************************\n",
      "inflation:\n",
      "['inflationary', 'core', 'cpi', 'expectations', 'accelerationist',\n",
      " 'reweighting', 'disinflation', 'unanchoring', 'pce', 'perpetuates']\n",
      "employment:\n",
      "['payroll', 'jobs', 'job', 'unemployment', 'manufacturing', 'dazzling', 'nonag',\n",
      " 'labor', 'epop', 'payrolls']\n",
      "interest:\n",
      "['rates', 'multiplication', 'funds', 'likeli', 'coax', 'antigrowth', 'hastened',\n",
      " 'exchange', 'iditions', 'nditions']\n",
      "price:\n",
      "['prices', 'stability', 'sulfurous', 'cpe', 'inflation', 'pans', 'preeminence',\n",
      " 'cornerstone', 'refiner', 'misrepresented']\n",
      "growth:\n",
      "['expansion', 'slower', 'grow', 'homing', 'slowing', 'histograms', 'pace',\n",
      " 'barreled', 'growing', 'someway']\n",
      "output:\n",
      "['gap', 'gdp', 'gaps', 'Î´yt', 'potential', 'productivity', 'distracting',\n",
      " 'misestimate', 'utilization', 'nairu']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "BBTSST_10_iter2_th80 Word Vectors\n",
      "********************************************************************************\n",
      "inflation:\n",
      "['median_and_trimmed_mean', 'median_and_sticky_price', 'core',\n",
      " 'solidly_grounded', 'inflationary',\n",
      " 'downside_risks_to_the_attainment_of_sustainable', 'accelerationist',\n",
      " 'exert_gravitational_pull', 'prove_costly_to_reverse',\n",
      " 'michigan_survey_philadelphia_fed']\n",
      "employment:\n",
      "['payroll_employment', 'jobs', 'unemployment', 'arizona_and_idaho', 'job',\n",
      " 'non_ag_employment', 'nonag_employment', 'fastest_growing_sectors',\n",
      " 'industrial_production', 'employ_ment']\n",
      "interest:\n",
      "['rates', 'risk_management_calculus', 'events_transpire',\n",
      " 'unprecedented_steepness_of_the_yield_curve', 'lack_of_deductibility',\n",
      " 'nditions_per_cent', 'wicksellian', 'reentry_problem',\n",
      " 'delinquency_and_foreclosure', 'instantaneous_real_forward']\n",
      "price:\n",
      "['prices', 'either_bumps_or_passes', 'low_skill_entry',\n",
      " 'korea_kospi_mexico_bolsa', 'index_brazil_bovespa', 'pans', 'case_schiller',\n",
      " 'sulfurous_crude', 'cpe', 'fomc_brazil_bovespa']\n",
      "growth:\n",
      "['expansion', 'homing', 'extraordinary_depth_of_the_recession',\n",
      " 'arithmetic_contribution_of_net_exports', 'fairly_lofty_levels',\n",
      " 'continues_to_expand_briskly', 'grow', 'greenbook_hypothesizes', 'barreled',\n",
      " 'rebuilding_in_the_gulf_region']\n",
      "output:\n",
      "['upwardly_revised_estimate', 'gdp', 'simply_denoting', 'yt_and_denotes',\n",
      " 'downwardly_revised_estimate', 'price_stab_ility',\n",
      " 'deliberate_or_opportunistic', 'weighted_sum_of_squared', 'potential',\n",
      " 'gliding']\n",
      "\n",
      "\n",
      "********************************************************************************\n",
      "glove-wiki-gigaword-300 Word Vectors\n",
      "********************************************************************************\n",
      "inflation:\n",
      "['inflationary', 'rate', 'rates', 'unemployment', 'growth', 'slowing',\n",
      " 'deflation', 'prices', 'accelerating', 'economy']\n",
      "employment:\n",
      "['jobs', 'unemployment', 'job', 'employers', 'benefits', 'wages', 'hiring',\n",
      " 'labor', 'workforce', 'jobless']\n",
      "interest:\n",
      "['rates', 'borrowing', 'concern', 'rate', 'raise', 'investors', 'inflation',\n",
      " 'lending', 'concerns', 'rise']\n",
      "price:\n",
      "['prices', 'cost', 'market', 'costs', 'higher', 'value', 'pricing', 'share',\n",
      " 'stock', 'premium']\n",
      "growth:\n",
      "['economic', 'economy', 'slowing', 'increase', 'gdp', 'decline', 'rise',\n",
      " 'recovery', 'slowdown', 'grow']\n",
      "output:\n",
      "['production', 'exports', 'consumption', 'increase', 'outputs', 'gdp',\n",
      " 'barrels', 'opec', 'export', 'exporting']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(width=80, compact=True)\n",
    "keywords = ['inflation','employment','interest','price','growth','output']\n",
    "for idx,model in enumerate(models):\n",
    "    print(\"*\"*80)\n",
    "    print(f\"{model_title[idx]} Word Vectors\")\n",
    "    print(\"*\"*80)\n",
    "    for key in keywords:\n",
    "        msw = [v[0] for v in model.most_similar(key)]\n",
    "        print(f\"{key}:\")\n",
    "        pp.pprint(msw)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex Export of results\n",
    "for idx,model in enumerate(models):\n",
    "    fulldata =pd.DataFrame([])\n",
    "    for key in keywords:\n",
    "        msw = [v[0] for v in model.most_similar(key)]\n",
    "        data = pd.DataFrame(msw,columns=[key])\n",
    "        fulldata = pd.concat([data,fulldata],axis=1)\n",
    "         \n",
    "    #print(fulldata.to_latex())\n",
    "    fulldata.to_latex(f\"{OVERLEAF}/emb_{model_title[idx]}.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Model Training\n",
    "\n",
    "Model choices:\n",
    "1. Jointly trained word embeddings. \n",
    "2. Manually trained word embeddings\n",
    "3. Pre-trained word embeddings\n",
    "\n",
    "Hyperparameters: All set at the default values of Dieng, Ruiz, Blei (2019)\n",
    "\n",
    "Model training is done in separate file.\n",
    "\n",
    "In Section ```#4``` the evaluation is executed for the three models described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model: etm_fomc_joint_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1\n",
      "\n",
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on FOMC_JOINT with the following settings: Namespace(anneal_lr=0, batch_size=1000, bow_norm=1, clip=0.0, data_path='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//data/SPEAKERS_10_iter2_th80', dataset='fomc_joint', emb_path='data/20ng_embeddings.txt', emb_size=300, enc_drop=0.0, epochs=20, eval_batch_size=1000, load_from='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//results/etm_fomc_joint_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1', log_interval=2, lr=0.005, lr_factor=4.0, mode='eval', nonmono=10, num_docs_test=268, num_docs_test_1=268, num_docs_test_2=268, num_docs_train=2480, num_docs_valid=143, num_topics=10, num_words=10, optimizer='adam', rho_size=300, save_path='./results', seed=2019, t_hidden_size=800, tc=1, td=1, theta_act='relu', train_embeddings=1, visualize_every=10, vocab_size=2842, wdecay=1.2e-06)\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=2842, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=2842, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 859.3\n",
      "Topic coherence is: 0.24820798288706566\n",
      "Topic diversity is: 0.372\n",
      "\n",
      "The 10 most used topics are [6 2 7 3 8 1 4 9 5 0]\n",
      "\n",
      "\n",
      "Topic 0: ['support', 'move', 'agree', 'rate', 'point', 'funds', 'make', 'policy', 'asymmetric']\n",
      "Topic 1: ['inflation', 'rate', 'policy', 'market', 'point', 'economy', 'rates', 'funds', 'basis']\n",
      "Topic 2: ['inflation', 'policy', 'rate', 'growth', 'market', 'funds', 'rates', 'economic', 'risks']\n",
      "Topic 3: ['move', 'time', 'rate', 'point', 'people', 'lot', 'inflation', 'make', 'policy']\n",
      "Topic 4: ['support', 'funds', 'agree', 'move', 'rate', 'language', 'alternative', 'point', 'policy']\n",
      "Topic 5: ['support', 'move', 'funds', 'agree', 'rate', 'point', 'language', 'alternative', 'policy']\n",
      "Topic 6: ['rate', 'significant', 'economy', 'data', 'productivity', 'evidence', 'capital', 'move', 'growth']\n",
      "Topic 7: ['lot', 'people', 'things', 'time', 'economy', 'inflation', 'move', 'good', 'real']\n",
      "Topic 8: ['rate', 'policy', 'inflation', 'funds', 'point', 'market', 'basis', 'time', 'rates']\n",
      "Topic 9: ['price', 'reserve', 'growth', 'conditions', 'economic', 'monetary', 'consistent', 'funds', 'federal']\n",
      "\n",
      "\n",
      "ETM embeddings...\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: inflation .. etm neighbors: ['inflation', 'market', 'economy', 'percent', 'forecast', 'real', 'lower', 'interest', 'current', 'rate', 'point', 'policy', 'basis', 'year', 'time', 'end', 'rates', 'view', 'action', 'points']\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: employment .. etm neighbors: ['employment', 'real', 'interest', 'past', 'level', 'high', 'forecast', 'low', 'economy', 'years', 'expect', 'part', 'current', 'effect', 'credit', 'inflation', 'year', 'recent', 'end', 'market']\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: interest .. etm neighbors: ['interest', 'inflation', 'real', 'year', 'forecast', 'action', 'expectations', 'market', 'current', 'economy', 'low', 'end', 'expect', 'uncertainty', 'years', 'situation', 'raise', 'time', 'high', 'past']\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: price .. etm neighbors: ['price', 'economic', 'consistent', 'financial', 'coming', 'outlook', 'domestic', 'greater', 'modest', 'weakness', 'degree', 'growth', 'business', 'strength', 'underlying', 'period', 'federal', 'upside', 'exchange', 'markets']\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: growth .. etm neighbors: ['growth', 'markets', 'monetary', 'rates', 'financial', 'increase', 'period', 'rate', 'policy', 'continue', 'economic', 'risks', 'funds', 'meeting', 'balance', 'price', 'coming', 'percent', 'information', 'outlook']\n",
      "vectors:  (2842, 300)\n",
      "query:  (300,)\n",
      "word: output .. etm neighbors: ['output', 'unemployment', 'effects', 'demand', 'potential', 'response', 'relative', 'prices', 'decline', 'core', 'bottom', 'upward', 'supply', 'staff', 'labor', 'spending', 'aggregate', 'nominal', 'rise', 'downward']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joint training of embeddings\n",
    "model = \"etm_fomc_joint_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1\"\n",
    "print(f\"Evaluate model: {model}\")\n",
    "print(os.popen(f'python main.py --mode eval --dataset fomc_joint --data_path {DATAPATH}/data/SPEAKERS_10_iter2_th80 --num_topics 10 --train_embeddings 1 --tc 1 --td 1 --load_from {DATAPATH}/results/{model}').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model: etm_fomc_pre_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0\n",
      "\n",
      "\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on FOMC_PRE with the following settings: Namespace(anneal_lr=0, batch_size=1000, bow_norm=1, clip=0.0, data_path='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//data/SPEAKERS_10_iter2_thinf', dataset='fomc_pre', emb_path='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//embeddings/BBTSST_10_iter2_thinf_emb', emb_size=300, embeddings_dim=torch.Size([3240, 300]), enc_drop=0.0, epochs=20, eval_batch_size=1000, load_from='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//results/etm_fomc_pre_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0', log_interval=2, lr=0.005, lr_factor=4.0, mode='eval', nonmono=10, num_docs_test=261, num_docs_test_1=261, num_docs_test_2=261, num_docs_train=2477, num_docs_valid=152, num_topics=10, num_words=10, optimizer='adam', rho_size=300, save_path='./results', seed=2019, t_hidden_size=800, tc=1, td=1, theta_act='relu', train_embeddings=0, visualize_every=10, vocab_size=3240, wdecay=1.2e-06)\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=3240, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 1029.2\n",
      "Topic coherence is: 0.2602413485828389\n",
      "Topic diversity is: 0.64\n",
      "\n",
      "The 10 most used topics are [8 6 4 5 2 0 9 7 3 1]\n",
      "\n",
      "\n",
      "Topic 0: ['inflation', 'economy', 'policy', 'monetary', 'real', 'view', 'rates', 'growth', 'forecast']\n",
      "Topic 1: ['support', 'asymmetric', 'alternative', 'directive', 'language', 'symmetric', 'favor', 'prefer', 'recommendation']\n",
      "Topic 2: ['statement', 'basis', 'point', 'policy', 'make', 'meeting', 'points', 'made', 'agree']\n",
      "Topic 3: ['move', 'point', 'basis', 'funds', 'rate', 'moving', 'points', 'million', 'borrowing']\n",
      "Topic 4: ['issue', 'change', 'question', 'back', 'make', 'move', 'time', 'find', 'put']\n",
      "Topic 5: ['rate', 'funds', 'percent', 'rates', 'interest', 'money', 'people', 'basis', 'period']\n",
      "Topic 6: ['policy', 'rate', 'market', 'funds', 'inflation', 'tightening', 'current', 'markets', 'basis']\n",
      "Topic 7: ['risks', 'risk', 'agree', 'important', 'concern', 'concerned', 'economy', 'situation', 'concerns']\n",
      "Topic 8: ['data', 'growth', 'labor', 'economy', 'productivity', 'real', 'rate', 'demand', 'evidence']\n",
      "Topic 9: ['risks', 'growth', 'economic', 'financial', 'reserve', 'conditions', 'federal', 'consistent', 'price']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained embeddings\n",
    "model = \"etm_fomc_pre_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0\"\n",
    "print(f\"Evaluate model: {model}\")\n",
    "print(os.popen(f'python main.py --mode eval --dataset fomc_pre --data_path {DATAPATH}/data/SPEAKERS_10_iter2_thinf --num_topics 10 --emb_path {DATAPATH}/embeddings/BBTSST_10_iter2_thinf_emb --train_embeddings 0 --tc 1 --td 1 --load_from {DATAPATH}/results/{model}').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate model: etm_fomc_impemb_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0\n",
      "\n",
      "\n",
      "nonlabor not in the embeddings\n",
      "updrift not in the embeddings\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "Training an Embedded Topic Model on FOMC_IMPEMB with the following settings: Namespace(anneal_lr=0, batch_size=1000, bow_norm=1, clip=0.0, data_path='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//data/SPEAKERS_10_iter2_thinf', dataset='fomc_impemb', emb_path='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//embeddings/preSPEAKERS_10_iter2_thinf', emb_size=300, embeddings_dim=torch.Size([3240, 300]), enc_drop=0.0, epochs=20, eval_batch_size=1000, load_from='/Users/olivergiesecke/Dropbox/MPCounterfactual/src/etm//results/etm_fomc_impemb_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0', log_interval=2, lr=0.005, lr_factor=4.0, mode='eval', nonmono=10, num_docs_test=261, num_docs_test_1=261, num_docs_test_2=261, num_docs_train=2477, num_docs_valid=152, num_topics=10, num_words=10, optimizer='adam', rho_size=300, save_path='./results', seed=2019, t_hidden_size=800, tc=1, td=1, theta_act='relu', train_embeddings=0, visualize_every=10, vocab_size=3240, wdecay=1.2e-06)\n",
      "=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n",
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.0, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=3240, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n",
      "****************************************************************************************************\n",
      "TEST Doc Completion PPL: 1157.3\n",
      "Topic coherence is: 0.25764051567825824\n",
      "Topic diversity is: 0.808\n",
      "\n",
      "The 10 most used topics are [9 5 7 6 2 8 1 3 0 4]\n",
      "\n",
      "\n",
      "Topic 0: ['support', 'point', 'make', 'good', 'lot', 'statement', 'concerned', 'feel', 'strong']\n",
      "Topic 1: ['agree', 'alternative', 'prefer', 'favor', 'reason', 'sentence', 'side', 'table', 'staff']\n",
      "Topic 2: ['policy', 'market', 'risks', 'markets', 'risk', 'statement', 'recent', 'uncertainty', 'potential']\n",
      "Topic 3: ['move', 'asymmetric', 'meeting', 'directive', 'language', 'change', 'symmetric', 'time', 'intermeeting']\n",
      "Topic 4: ['economy', 'price', 'economic', 'financial', 'monetary', 'growth', 'markets', 'reserve', 'conditions']\n",
      "Topic 5: ['time', 'question', 'move', 'back', 'make', 'issue', 'people', 'point', 'part']\n",
      "Topic 6: ['basis', 'real', 'points', 'terms', 'sense', 'stock', 'discount', 'view', 'fairly']\n",
      "Topic 7: ['inflation', 'percent', 'growth', 'rates', 'rate', 'forecast', 'current', 'lower', 'expectations']\n",
      "Topic 8: ['funds', 'rate', 'balance', 'tightening', 'federal', 'action', 'rates', 'easing', 'move']\n",
      "Topic 9: ['data', 'productivity', 'economy', 'labor', 'demand', 'capital', 'prices', 'real', 'costs']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Glove pre-trained embeddings\n",
    "model = \"etm_fomc_impemb_K_10_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0\"\n",
    "print(f\"Evaluate model: {model}\")\n",
    "print(os.popen(f'python main.py --mode eval --dataset fomc_impemb --data_path {DATAPATH}/data/SPEAKERS_10_iter2_thinf --num_topics 10 --emb_path {DATAPATH}/embeddings/preSPEAKERS_10_iter2_thinf --train_embeddings 0 --tc 1 --td 1 --load_from {DATAPATH}/results/{model}').read())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
